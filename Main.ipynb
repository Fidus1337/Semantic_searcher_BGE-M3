{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233c45c1-922e-4c0d-b136-d6c9b9d9b664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\flays\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "============================================================\n",
      "üîç GPU CHECK\n",
      "============================================================\n",
      "\n",
      "üì¶ PyTorch version: 2.6.0+cu124\n",
      "üîß CUDA available: True\n",
      "üéÆ CUDA version: 12.4\n",
      "üìä GPU count: 1\n",
      "\n",
      "   GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "   Memory: 4.3 GB\n",
      "   Compute capability: 8.6\n",
      "\n",
      "üíæ Current VRAM usage:\n",
      "   Allocated: 0.00 GB\n",
      "   Cached: 0.00 GB\n",
      "\n",
      "üì¶ FAISS GPU support: False\n",
      "============================================================\n",
      "\n",
      "üìÇ LOADING DATA:\n",
      "\n",
      "üîç COLUMN ANALYSIS:\n",
      "Competitor file (df_competitor): ['id', 'name']\n",
      "Our file (df_our):               ['id', 'name']\n",
      "------------------------------------------------------------\n",
      "============================================================\n",
      "üöÄ Initializing ProductMatcherGPU\n",
      "============================================================\n",
      "üñ•Ô∏è  Device: cuda:0\n",
      "üéÆ GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "üíæ VRAM: 4.3 GB\n",
      "üîß FAISS-GPU: ‚ùå Not installed (Using CPU)\n",
      "‚ö° Precision: FP16\n",
      "\n",
      "üì• Loading encoder: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Encoder on: cuda:0\n",
      "\n",
      "üì• Loading reranker: BAAI/bge-reranker-v2-m3\n",
      "   ‚úÖ Reranker on: cuda:0\n",
      "\n",
      "üíæ GPU Memory: 2.27 / 4.3 GB (reserved: 2.28 GB)\n",
      "\n",
      "‚úÖ Initialization complete!\n",
      "============================================================\n",
      "\n",
      "======================================================================\n",
      "üöÄ MATCHING PRODUCTS\n",
      "======================================================================\n",
      "\n",
      "üíæ GPU Memory: 2.27 / 4.3 GB (reserved: 2.28 GB)\n",
      "‚ö†Ô∏è Cache not found\n",
      "\n",
      "============================================================\n",
      "üíæ Creating Index (500 items)\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Cleaning texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 28101.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ Creating embeddings...\n",
      "   Batch size: 8 (free VRAM: 2.0 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1581b642e6dc4eb4bb7f36c7cc02ea3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£ Creating FAISS index...\n",
      "   üìä FAISS on CPU\n",
      "\n",
      "4Ô∏è‚É£ Saving...\n",
      "‚úÖ Saved to cache_products/\n",
      "\n",
      "üíæ GPU Memory: 2.28 / 4.3 GB (reserved: 2.31 GB)\n",
      "\n",
      "üì¶ Our products: 500\n",
      "üì¶ Competitor products: 500\n",
      "\n",
      "--------------------------------------------------\n",
      "üìù Stage 1: Preparing competitor products\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 65060.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating embeddings...\n",
      "   Batch size: 8 (free VRAM: 2.0 GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54389fa9dfd64e3ea817533524bb8d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "üîç Stage 2: Retrieval (Top-5)\n",
      "--------------------------------------------------\n",
      "   ‚úÖ Candidates found for 500 items\n",
      "\n",
      "--------------------------------------------------\n",
      "üéØ Stage 3: Reranking\n",
      "--------------------------------------------------\n",
      "   Reranking 2500 pairs (batch=16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06fa0b079ab495f87cd7a26897afad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "üìä Stage 4: Building results\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Done!\n",
      "   Matched: 500 (100.0%)\n",
      "   Low confidence: 0 (0.0%)\n",
      "\n",
      "üíæ GPU Memory: 2.28 / 4.3 GB (reserved: 2.33 GB)\n",
      "\n",
      "======================================================================\n",
      "üìä RESULTS ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìà Statistics:\n",
      "   Total: 500\n",
      "   ‚úÖ Matched: 500 (100.0%)\n",
      "   ‚ö†Ô∏è  Low confidence: 0 (0.0%)\n",
      "\n",
      "üìâ Rerank score:\n",
      "   Mean: 0.9911\n",
      "   Median: 0.9995\n",
      "   Min/Max: 0.7734 / 1.0000\n",
      "\n",
      "üìä Distribution:\n",
      "   >= 0.9: 489 (97.8%)\n",
      "   >= 0.7: 500 (100.0%)\n",
      "   >= 0.5: 500 (100.0%)\n",
      "   >= 0.3: 500 (100.0%)\n",
      "\n",
      "üìã Top-5 Best Matches:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. Score: 1.0000\n",
      "   Competitor: Pantene ProV –®–∞–º–ø—É–Ω—å –æ–±—ä–µ–º 250–º–ª\n",
      "   Our:        Pantene Pro-V Shamp –æ–±—ä–µ–º 250–º–ª\n",
      "\n",
      "2. Score: 1.0000\n",
      "   Competitor: L'Oreal Elseve –®–ê–ú–ü–£–ù–¨ –ø—Ä–æ—Ç–∏–≤ –≤—ã–ø–∞–¥–µ–Ω–∏—è 400–º–ª\n",
      "   Our:        Shamp –≠–ª—å—Å–µ–≤ - –ø—Ä–æ—Ç–∏–≤ –≤—ã–ø–∞–¥–µ–Ω–∏—è / 400–º–ª\n",
      "\n",
      "3. Score: 1.0000\n",
      "   Competitor: –®–ê–ú–ü–£–ù–¨ Elseve –¥–ª—è –æ–∫—Ä–∞—à–µ–Ω–Ω—ã—Ö –≤–æ–ª–æ—Å 250–º–ª\n",
      "   Our:        –¥–ª—è –æ–∫—Ä–∞—à–µ–Ω–Ω—ã—Ö –≤–æ–ª–æ—Å –ó–∞—Å—ñ–± –¥–ª—è –º–∏—Ç—Ç—è –≤–æ–ª–æ—Å—Å—è Elseve 250–º–ª\n",
      "\n",
      "4. Score: 1.0000\n",
      "   Competitor: –®–ê–ú–ü–£–ù–¨ SCHAUMA \"–¥–ª—è –º—É–∂—á–∏–Ω\" 400–º–ª\n",
      "   Our:        –¥–ª—è –º—É–∂—á–∏–Ω –®–∞–º–ø—É–Ω—å –¥–ª—è –≤–æ–ª–æ—Å—Å—è SCHAUMA 400–º–ª\n",
      "\n",
      "5. Score: 1.0000\n",
      "   Competitor: –®–∞–º–ø—É–Ω—å –¥/–≤–æ–ª–æ—Å –®–∞—É–º–∞ (fresh it up) 250–º–ª\n",
      "   Our:        –®–∞—É–º–∞ | SHAMPOO | fresh it up | 250–º–ª\n",
      "\n",
      "üíæ Results saved to file: results_gpu_matched.csv\n",
      "üßπ Clearing models...\n",
      "   GPU memory: 2.28 GB\n",
      "‚úÖ Cleared\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Product Matcher v4 - GPU Optimized (RTX 3050 Ready)\n",
    "===================================================\n",
    "- Fixed deprecated parameters\n",
    "- Optimized for 4GB VRAM\n",
    "- FAISS runs on CPU (GPU version optional)\n",
    "- Encoder + Reranker run on GPU\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import faiss\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def check_gpu_status():\n",
    "    \"\"\"Checks and prints GPU status.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üîç GPU CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nüì¶ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"üîß CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üéÆ CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"üìä GPU count: {torch.cuda.device_count()}\")\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"\\n   GPU {i}: {props.name}\")\n",
    "            print(f\"   Memory: {props.total_memory / 1e9:.1f} GB\")\n",
    "            print(f\"   Compute capability: {props.major}.{props.minor}\")\n",
    "        \n",
    "        print(f\"\\nüíæ Current VRAM usage:\")\n",
    "        print(f\"   Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        print(f\"   Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  GPU unavailable! Using CPU.\")\n",
    "    \n",
    "    print(f\"\\nüì¶ FAISS GPU support: {hasattr(faiss, 'StandardGpuResources')}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return torch.cuda.is_available()\n",
    "\n",
    "\n",
    "class ProductMatcherGPU:\n",
    "    \"\"\"\n",
    "    GPU-optimized product matcher:\n",
    "    1. Retrieval: BGE-M3 (GPU) + FAISS -> Top-K candidates\n",
    "    2. Reranking: BGE-reranker-v2-m3 (GPU) -> Best candidate\n",
    "    \"\"\"\n",
    "    \n",
    "    _encoder_instance = None\n",
    "    _reranker_instance = None\n",
    "    \n",
    "    def __init__(self, \n",
    "                 encoder_model: str = 'BAAI/bge-m3',\n",
    "                 reranker_model: str = 'BAAI/bge-reranker-v2-m3',\n",
    "                 cache_dir: str = './cache',\n",
    "                 use_fp16: bool = True,\n",
    "                 gpu_id: int = 0,\n",
    "                 force_gpu: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_model: Model for embeddings\n",
    "            reranker_model: Model for reranking\n",
    "            cache_dir: Directory for cache storage\n",
    "            use_fp16: Use float16 to save VRAM\n",
    "            gpu_id: GPU ID to use\n",
    "            force_gpu: Raise error if GPU is not found\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gpu_available = torch.cuda.is_available()\n",
    "        self.gpu_id = gpu_id\n",
    "        \n",
    "        if force_gpu and not self.gpu_available:\n",
    "            raise RuntimeError(\"‚ùå GPU not found!\")\n",
    "        \n",
    "        # Initialize attributes\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.faiss_index: Optional[faiss.Index] = None\n",
    "        self.gpu_resources = None\n",
    "        self.eva_texts_clean: Optional[List[str]] = None\n",
    "        self.eva_df: Optional[pd.DataFrame] = None\n",
    "        \n",
    "        # Device setup\n",
    "        if self.gpu_available:\n",
    "            self.device = f'cuda:{gpu_id}'\n",
    "            torch.cuda.set_device(gpu_id)\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"üöÄ Initializing ProductMatcherGPU\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üñ•Ô∏è  Device: {self.device}\")\n",
    "        \n",
    "        if self.gpu_available:\n",
    "            props = torch.cuda.get_device_properties(gpu_id)\n",
    "            self.vram_total = props.total_memory / 1e9\n",
    "            print(f\"üéÆ GPU: {props.name}\")\n",
    "            print(f\"üíæ VRAM: {self.vram_total:.1f} GB\")\n",
    "            \n",
    "            # FAISS-GPU (Optional)\n",
    "            self.faiss_gpu_available = hasattr(faiss, 'StandardGpuResources')\n",
    "            if self.faiss_gpu_available:\n",
    "                try:\n",
    "                    self.gpu_resources = faiss.StandardGpuResources()\n",
    "                    self.gpu_resources.setTempMemory(256 * 1024 * 1024)  # 256MB\n",
    "                    print(f\"üîß FAISS-GPU: ‚úÖ Available\")\n",
    "                except:\n",
    "                    self.faiss_gpu_available = False\n",
    "                    print(f\"üîß FAISS-GPU: ‚ùå Initialization error\")\n",
    "            else:\n",
    "                print(f\"üîß FAISS-GPU: ‚ùå Not installed (Using CPU)\")\n",
    "        else:\n",
    "            self.vram_total = 0\n",
    "            self.faiss_gpu_available = False\n",
    "        \n",
    "        # Dtype\n",
    "        if use_fp16 and self.gpu_available:\n",
    "            self.model_dtype = torch.float16\n",
    "            print(\"‚ö° Precision: FP16\")\n",
    "        else:\n",
    "            self.model_dtype = torch.float32\n",
    "            print(\"üìä Precision: FP32\")\n",
    "        \n",
    "        # Load models\n",
    "        self._load_encoder(encoder_model)\n",
    "        self._load_reranker(reranker_model)\n",
    "        \n",
    "        if self.gpu_available:\n",
    "            self._print_gpu_memory()\n",
    "        \n",
    "        print(\"\\n‚úÖ Initialization complete!\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    def _load_encoder(self, model_name: str):\n",
    "        \"\"\"Load encoder onto GPU.\"\"\"\n",
    "        if ProductMatcherGPU._encoder_instance is None:\n",
    "            print(f\"\\nüì• Loading encoder: {model_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Modern method with dtype\n",
    "                ProductMatcherGPU._encoder_instance = SentenceTransformer(\n",
    "                    model_name,\n",
    "                    device=self.device,\n",
    "                    model_kwargs={\n",
    "                        'torch_dtype': self.model_dtype,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e1:\n",
    "                print(f\"   ‚ö†Ô∏è Attempt 1 failed: {e1}\")\n",
    "                try:\n",
    "                    # Fallback without kwargs\n",
    "                    ProductMatcherGPU._encoder_instance = SentenceTransformer(\n",
    "                        model_name,\n",
    "                        device=self.device\n",
    "                    )\n",
    "                    if self.model_dtype == torch.float16 and self.gpu_available:\n",
    "                        ProductMatcherGPU._encoder_instance.half()\n",
    "                except Exception as e2:\n",
    "                    print(f\"   ‚ùå Load error: {e2}\")\n",
    "                    raise\n",
    "            \n",
    "            if self.gpu_available:\n",
    "                try:\n",
    "                    dev = next(ProductMatcherGPU._encoder_instance.parameters()).device\n",
    "                    print(f\"   ‚úÖ Encoder on: {dev}\")\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            print(\"‚úÖ Encoder already loaded\")\n",
    "        \n",
    "        self.encoder = ProductMatcherGPU._encoder_instance\n",
    "    \n",
    "    def _load_reranker(self, model_name: str):\n",
    "        \"\"\"Load reranker onto GPU.\"\"\"\n",
    "        if ProductMatcherGPU._reranker_instance is None:\n",
    "            print(f\"\\nüì• Loading reranker: {model_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Modern method with model_kwargs\n",
    "                ProductMatcherGPU._reranker_instance = CrossEncoder(\n",
    "                    model_name,\n",
    "                    max_length=512,\n",
    "                    device=self.device,\n",
    "                    model_kwargs={'torch_dtype': self.model_dtype}\n",
    "                )\n",
    "            except Exception as e1:\n",
    "                print(f\"   ‚ö†Ô∏è Attempt 1 failed: {e1}\")\n",
    "                try:\n",
    "                    # Fallback\n",
    "                    ProductMatcherGPU._reranker_instance = CrossEncoder(\n",
    "                        model_name,\n",
    "                        max_length=512,\n",
    "                        device=self.device\n",
    "                    )\n",
    "                    if self.model_dtype == torch.float16 and self.gpu_available:\n",
    "                        ProductMatcherGPU._reranker_instance.model.half()\n",
    "                except Exception as e2:\n",
    "                    print(f\"   ‚ùå Load error: {e2}\")\n",
    "                    raise\n",
    "            \n",
    "            if self.gpu_available:\n",
    "                try:\n",
    "                    dev = next(ProductMatcherGPU._reranker_instance.model.parameters()).device\n",
    "                    print(f\"   ‚úÖ Reranker on: {dev}\")\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            print(\"‚úÖ Reranker already loaded\")\n",
    "        \n",
    "        self.reranker = ProductMatcherGPU._reranker_instance\n",
    "    \n",
    "    def _print_gpu_memory(self):\n",
    "        \"\"\"Print GPU memory usage.\"\"\"\n",
    "        if self.gpu_available:\n",
    "            allocated = torch.cuda.memory_allocated(self.gpu_id) / 1e9\n",
    "            reserved = torch.cuda.memory_reserved(self.gpu_id) / 1e9\n",
    "            print(f\"\\nüíæ GPU Memory: {allocated:.2f} / {self.vram_total:.1f} GB (reserved: {reserved:.2f} GB)\")\n",
    "    \n",
    "    def _get_free_vram(self) -> float:\n",
    "        \"\"\"Get free VRAM in GB.\"\"\"\n",
    "        if not self.gpu_available:\n",
    "            return 0\n",
    "        allocated = torch.cuda.memory_allocated(self.gpu_id) / 1e9\n",
    "        return self.vram_total - allocated\n",
    "    \n",
    "    @classmethod\n",
    "    def clear_models(cls):\n",
    "        \"\"\"Clear models and GPU memory.\"\"\"\n",
    "        print(\"üßπ Clearing models...\")\n",
    "        cls._encoder_instance = None\n",
    "        cls._reranker_instance = None\n",
    "        gc.collect()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            print(f\"   GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "        \n",
    "        print(\"‚úÖ Cleared\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Text Cleaning\n",
    "    # =========================================================================\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Text normalization.\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower().strip()\n",
    "        \n",
    "        # Units normalization (Cyrillic to standard)\n",
    "        replacements = [\n",
    "            (r'(\\d+)\\s*–º–ª\\b', r'\\1–º–ª'),\n",
    "            (r'(\\d+)\\s*ml\\b', r'\\1–º–ª'),\n",
    "            (r'(\\d+)\\s*–≥\\b', r'\\1–≥'),\n",
    "            (r'(\\d+)\\s*g\\b', r'\\1–≥'),\n",
    "            (r'(\\d+)\\s*–∫–≥\\b', r'\\1–∫–≥'),\n",
    "            (r'(\\d+)\\s*kg\\b', r'\\1–∫–≥'),\n",
    "            (r'(\\d+)\\s*–ª\\b', r'\\1–ª'),\n",
    "            (r'(\\d+)\\s*l\\b', r'\\1–ª'),\n",
    "            (r'(\\d+)\\s*—à—Ç\\b', r'\\1—à—Ç'),\n",
    "        ]\n",
    "        \n",
    "        for pattern, repl in replacements:\n",
    "            text = re.sub(pattern, repl, text)\n",
    "        \n",
    "        # Cleanup special characters\n",
    "        text = re.sub(r'[^\\w\\s\\-\\.,/]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def clean_texts_batch(self, texts: List[str], desc: str = \"Cleaning\") -> List[str]:\n",
    "        \"\"\"Batch text cleaning.\"\"\"\n",
    "        return [self.clean_text(t) for t in tqdm(texts, desc=desc)]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Embeddings\n",
    "    # =========================================================================\n",
    "    \n",
    "    def create_embeddings(self, \n",
    "                          texts: List[str], \n",
    "                          batch_size: int = 8,\n",
    "                          show_progress: bool = True) -> np.ndarray:\n",
    "        \"\"\"Create embeddings (GPU).\"\"\"\n",
    "        \n",
    "        # Auto-adjust batch_size for RTX 3050\n",
    "        if self.gpu_available:\n",
    "            free_vram = self._get_free_vram()\n",
    "            if free_vram < 1.5:\n",
    "                batch_size = min(batch_size, 4)\n",
    "            elif free_vram < 2.0:\n",
    "                batch_size = min(batch_size, 8)\n",
    "            elif free_vram < 3.0:\n",
    "                batch_size = min(batch_size, 16)\n",
    "            \n",
    "            print(f\"   Batch size: {batch_size} (free VRAM: {free_vram:.1f} GB)\")\n",
    "        \n",
    "        embeddings = self.encoder.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=show_progress,\n",
    "            normalize_embeddings=True,\n",
    "            convert_to_numpy=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        return embeddings.astype('float32')\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FAISS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def create_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:\n",
    "        \"\"\"Create FAISS index.\"\"\"\n",
    "        dimension = embeddings.shape[1]\n",
    "        \n",
    "        # CPU index (more reliable)\n",
    "        index = faiss.IndexFlatIP(dimension)\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        # GPU index if available and memory permits\n",
    "        if self.faiss_gpu_available and self.gpu_resources and self._get_free_vram() > 0.5:\n",
    "            try:\n",
    "                gpu_index = faiss.index_cpu_to_gpu(self.gpu_resources, self.gpu_id, index)\n",
    "                print(f\"   ‚úÖ FAISS on GPU\")\n",
    "                return gpu_index\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è FAISS on GPU failed: {e}\")\n",
    "        \n",
    "        print(f\"   üìä FAISS on CPU\")\n",
    "        return index\n",
    "    \n",
    "    def _index_to_cpu(self, index: faiss.Index) -> faiss.Index:\n",
    "        \"\"\"Transfer index to CPU.\"\"\"\n",
    "        if hasattr(faiss, 'index_gpu_to_cpu'):\n",
    "            try:\n",
    "                return faiss.index_gpu_to_cpu(index)\n",
    "            except:\n",
    "                pass\n",
    "        return index\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Caching\n",
    "    # =========================================================================\n",
    "    \n",
    "    def _get_cache_path(self, name: str) -> Path:\n",
    "        return self.cache_dir / name\n",
    "    \n",
    "    def save_index(self, \n",
    "                   df: pd.DataFrame,\n",
    "                   text_column: str,\n",
    "                   cache_name: str = 'products'):\n",
    "        \"\"\"Create and save index.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"üíæ Creating Index ({len(df)} items)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Cleaning\n",
    "        print(\"\\n1Ô∏è‚É£ Cleaning texts...\")\n",
    "        texts_clean = self.clean_texts_batch(df[text_column].tolist())\n",
    "        \n",
    "        # Embeddings\n",
    "        print(\"\\n2Ô∏è‚É£ Creating embeddings...\")\n",
    "        embeddings = self.create_embeddings(texts_clean)\n",
    "        \n",
    "        # Index\n",
    "        print(\"\\n3Ô∏è‚É£ Creating FAISS index...\")\n",
    "        index = self.create_faiss_index(embeddings)\n",
    "        \n",
    "        # Saving\n",
    "        print(\"\\n4Ô∏è‚É£ Saving...\")\n",
    "        cpu_index = self._index_to_cpu(index)\n",
    "        \n",
    "        np.save(self._get_cache_path(f'{cache_name}_embeddings.npy'), embeddings)\n",
    "        pd.DataFrame({'text_clean': texts_clean}).to_parquet(\n",
    "            self._get_cache_path(f'{cache_name}_texts.parquet')\n",
    "        )\n",
    "        faiss.write_index(cpu_index, str(self._get_cache_path(f'{cache_name}_index.faiss')))\n",
    "        df.to_parquet(self._get_cache_path(f'{cache_name}_products.parquet'))\n",
    "        \n",
    "        print(f\"‚úÖ Saved to {self.cache_dir}/\")\n",
    "        \n",
    "        self.faiss_index = index\n",
    "        self.eva_texts_clean = texts_clean\n",
    "        self.eva_df = df\n",
    "        \n",
    "        if self.gpu_available:\n",
    "            self._print_gpu_memory()\n",
    "        \n",
    "        return index, texts_clean\n",
    "    \n",
    "    def load_index(self, cache_name: str = 'products') -> bool:\n",
    "        \"\"\"Load index from cache.\"\"\"\n",
    "        paths = [\n",
    "            self._get_cache_path(f'{cache_name}_embeddings.npy'),\n",
    "            self._get_cache_path(f'{cache_name}_texts.parquet'),\n",
    "            self._get_cache_path(f'{cache_name}_index.faiss'),\n",
    "            self._get_cache_path(f'{cache_name}_products.parquet'),\n",
    "        ]\n",
    "        \n",
    "        if not all(p.exists() for p in paths):\n",
    "            print(\"‚ö†Ô∏è Cache not found\")\n",
    "            return False\n",
    "        \n",
    "        print(\"\\nüìÇ Loading from cache...\")\n",
    "        \n",
    "        self.faiss_index = faiss.read_index(str(paths[2]))\n",
    "        self.eva_texts_clean = pd.read_parquet(paths[1])['text_clean'].tolist()\n",
    "        self.eva_df = pd.read_parquet(paths[3])\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {self.faiss_index.ntotal} items\")\n",
    "        \n",
    "        if self.gpu_available:\n",
    "            self._print_gpu_memory()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Search and Reranking\n",
    "    # =========================================================================\n",
    "    \n",
    "    def search_candidates(self, \n",
    "                          query_embeddings: np.ndarray,\n",
    "                          top_k: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Search candidates.\"\"\"\n",
    "        scores, indices = self.faiss_index.search(query_embeddings.astype('float32'), top_k)\n",
    "        return indices, scores\n",
    "    \n",
    "    def rerank_batch(self,\n",
    "                     query_texts: List[str],\n",
    "                     candidates_indices: np.ndarray,\n",
    "                     batch_size: int = 16) -> List[Tuple[int, float, int]]:\n",
    "        \"\"\"Rerank candidates.\"\"\"\n",
    "        \n",
    "        # Collect pairs\n",
    "        all_pairs = []\n",
    "        pair_info = []\n",
    "        \n",
    "        for q_idx, (query, cand_indices) in enumerate(zip(query_texts, candidates_indices)):\n",
    "            for pos, idx in enumerate(cand_indices):\n",
    "                if idx >= 0:\n",
    "                    all_pairs.append([query, self.eva_texts_clean[idx]])\n",
    "                    pair_info.append((q_idx, pos, idx))\n",
    "        \n",
    "        # Auto-adjust batch_size\n",
    "        if self.gpu_available:\n",
    "            free_vram = self._get_free_vram()\n",
    "            if free_vram < 1.0:\n",
    "                batch_size = min(batch_size, 8)\n",
    "            elif free_vram < 1.5:\n",
    "                batch_size = min(batch_size, 16)\n",
    "            elif free_vram < 2.0:\n",
    "                batch_size = min(batch_size, 32)\n",
    "        \n",
    "        print(f\"   Reranking {len(all_pairs)} pairs (batch={batch_size})...\")\n",
    "        \n",
    "        # Predict\n",
    "        all_scores = self.reranker.predict(\n",
    "            all_pairs, \n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Grouping\n",
    "        query_results = {}\n",
    "        for (q_idx, pos, idx), score in zip(pair_info, all_scores):\n",
    "            if q_idx not in query_results:\n",
    "                query_results[q_idx] = []\n",
    "            query_results[q_idx].append((idx, float(score), pos))\n",
    "        \n",
    "        # Select best for each\n",
    "        results = []\n",
    "        for q_idx in range(len(query_texts)):\n",
    "            if q_idx in query_results:\n",
    "                best = max(query_results[q_idx], key=lambda x: x[1])\n",
    "                results.append(best)\n",
    "            else:\n",
    "                results.append((-1, 0.0, -1))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Main Method\n",
    "    # =========================================================================\n",
    "    \n",
    "    def match_products(self,\n",
    "                       competitor_df: pd.DataFrame,\n",
    "                       competitor_col: str = 'name',\n",
    "                       our_df: Optional[pd.DataFrame] = None,\n",
    "                       our_col: str = 'name',\n",
    "                       top_k: int = 10,\n",
    "                       threshold: float = 0.5,\n",
    "                       encoder_batch: int = 8,\n",
    "                       reranker_batch: int = 16,\n",
    "                       use_cache: bool = True,\n",
    "                       cache_name: str = 'products') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Match products.\n",
    "        \n",
    "        Args:\n",
    "            competitor_df: Competitor DataFrame\n",
    "            competitor_col: Column with product name\n",
    "            our_df: Our DataFrame (or None if cached)\n",
    "            our_col: Column with product name\n",
    "            top_k: Number of candidates for reranking\n",
    "            threshold: Confidence threshold\n",
    "            encoder_batch: Batch size for encoder\n",
    "            reranker_batch: Batch size for reranker\n",
    "            use_cache: Use cache\n",
    "            cache_name: Cache name\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"üöÄ MATCHING PRODUCTS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if self.gpu_available:\n",
    "            self._print_gpu_memory()\n",
    "        \n",
    "        # 1. Indexing our products\n",
    "        if self.faiss_index is None:\n",
    "            if use_cache and self.load_index(cache_name):\n",
    "                pass\n",
    "            elif our_df is not None:\n",
    "                self.save_index(our_df, our_col, cache_name)\n",
    "            else:\n",
    "                raise ValueError(\"Provide our_df or use cache\")\n",
    "        \n",
    "        print(f\"\\nüì¶ Our products: {self.faiss_index.ntotal}\")\n",
    "        print(f\"üì¶ Competitor products: {len(competitor_df)}\")\n",
    "        \n",
    "        # 2. Competitor Processing\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"üìù Stage 1: Preparing competitor products\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        comp_texts = competitor_df[competitor_col].tolist()\n",
    "        comp_texts_clean = self.clean_texts_batch(comp_texts, \"Cleaning\")\n",
    "        \n",
    "        print(\"\\nCreating embeddings...\")\n",
    "        comp_embeddings = self.create_embeddings(comp_texts_clean, batch_size=encoder_batch)\n",
    "        \n",
    "        # 3. Retrieval\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(f\"üîç Stage 2: Retrieval (Top-{top_k})\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        cand_indices, cand_scores = self.search_candidates(comp_embeddings, top_k)\n",
    "        print(f\"   ‚úÖ Candidates found for {len(cand_indices)} items\")\n",
    "        \n",
    "        # 4. Reranking\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"üéØ Stage 3: Reranking\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rerank_results = self.rerank_batch(comp_texts_clean, cand_indices, batch_size=reranker_batch)\n",
    "        \n",
    "        # 5. Results\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"üìä Stage 4: Building results\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Determine column\n",
    "        if our_col in self.eva_df.columns:\n",
    "            eva_col = our_col\n",
    "        else:\n",
    "            eva_col = self.eva_df.columns[1]\n",
    "        \n",
    "        results = []\n",
    "        for comp_idx, (eva_idx, score, rank) in enumerate(rerank_results):\n",
    "            \n",
    "            status = \"matched\" if score >= threshold else \"low_confidence\"\n",
    "            \n",
    "            results.append({\n",
    "                'competitor_index': comp_idx,\n",
    "                'competitor_product': competitor_df.iloc[comp_idx][competitor_col],\n",
    "                'our_index': eva_idx if status == \"matched\" else None,\n",
    "                'our_product': self.eva_df.iloc[eva_idx][eva_col] if eva_idx >= 0 else None,\n",
    "                'retrieval_score': float(cand_scores[comp_idx][rank]) if rank >= 0 else 0.0,\n",
    "                'rerank_score': score,\n",
    "                'retrieval_rank': rank + 1 if rank >= 0 else -1,\n",
    "                'match_status': status\n",
    "            })\n",
    "        \n",
    "        result_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Statistics\n",
    "        matched = len(result_df[result_df['match_status'] == 'matched'])\n",
    "        low_conf = len(result_df[result_df['match_status'] == 'low_confidence'])\n",
    "        \n",
    "        print(f\"\\n‚úÖ Done!\")\n",
    "        print(f\"   Matched: {matched} ({matched/len(result_df)*100:.1f}%)\")\n",
    "        print(f\"   Low confidence: {low_conf} ({low_conf/len(result_df)*100:.1f}%)\")\n",
    "        \n",
    "        if self.gpu_available:\n",
    "            self._print_gpu_memory()\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def analyze_results(self, df: pd.DataFrame, show_examples: int = 5) -> Dict:\n",
    "        \"\"\"Analyze results.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"üìä RESULTS ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        total = len(df)\n",
    "        matched = df[df['match_status'] == 'matched']\n",
    "        low_conf = df[df['match_status'] == 'low_confidence']\n",
    "        \n",
    "        print(f\"\\nüìà Statistics:\")\n",
    "        print(f\"   Total: {total}\")\n",
    "        print(f\"   ‚úÖ Matched: {len(matched)} ({len(matched)/total*100:.1f}%)\")\n",
    "        print(f\"   ‚ö†Ô∏è  Low confidence: {len(low_conf)} ({len(low_conf)/total*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nüìâ Rerank score:\")\n",
    "        print(f\"   Mean: {df['rerank_score'].mean():.4f}\")\n",
    "        print(f\"   Median: {df['rerank_score'].median():.4f}\")\n",
    "        print(f\"   Min/Max: {df['rerank_score'].min():.4f} / {df['rerank_score'].max():.4f}\")\n",
    "        \n",
    "        print(f\"\\nüìä Distribution:\")\n",
    "        for t in [0.9, 0.7, 0.5, 0.3]:\n",
    "            count = len(df[df['rerank_score'] >= t])\n",
    "            print(f\"   >= {t}: {count} ({count/total*100:.1f}%)\")\n",
    "        \n",
    "        if show_examples > 0 and len(matched) > 0:\n",
    "            print(f\"\\nüìã Top-{show_examples} Best Matches:\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for i, (_, row) in enumerate(matched.nlargest(show_examples, 'rerank_score').iterrows(), 1):\n",
    "                print(f\"\\n{i}. Score: {row['rerank_score']:.4f}\")\n",
    "                print(f\"   Competitor: {row['competitor_product'][:65]}\")\n",
    "                print(f\"   Our:        {str(row['our_product'])[:65]}\")\n",
    "        \n",
    "        if show_examples > 0 and len(low_conf) > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  Worst {min(3, len(low_conf))}:\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for i, (_, row) in enumerate(low_conf.nsmallest(min(3, len(low_conf)), 'rerank_score').iterrows(), 1):\n",
    "                print(f\"\\n{i}. Score: {row['rerank_score']:.4f}\")\n",
    "                print(f\"   Competitor: {row['competitor_product'][:65]}\")\n",
    "                print(f\"   Best Guess: {str(row['our_product'])[:65]}\")\n",
    "        \n",
    "        return {\n",
    "            'total': total,\n",
    "            'matched': len(matched),\n",
    "            'low_confidence': len(low_conf),\n",
    "            'avg_score': df['rerank_score'].mean(),\n",
    "            'median_score': df['rerank_score'].median()\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# QUICK START\n",
    "# =============================================================================\n",
    "\n",
    "def quick_match(competitor_csv: str,\n",
    "                our_csv: str,\n",
    "                output_csv: str = 'matches.csv',\n",
    "                competitor_col: str = 'name',\n",
    "                our_col: str = 'name',\n",
    "                threshold: float = 0.5,\n",
    "                top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Quick match from CSV files.\n",
    "    \n",
    "    Example:\n",
    "        results = quick_match('competitor.csv', 'our_products.csv')\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading data...\")\n",
    "    competitor_df = pd.read_csv(competitor_csv)\n",
    "    our_df = pd.read_csv(our_csv)\n",
    "    \n",
    "    print(f\"   Competitor: {len(competitor_df)} items\")\n",
    "    print(f\"   Our: {len(our_df)} items\")\n",
    "    \n",
    "    # Matcher\n",
    "    matcher = ProductMatcherGPU(use_fp16=True, force_gpu=False)\n",
    "    \n",
    "    # Matching (parameters for 4GB VRAM)\n",
    "    results = matcher.match_products(\n",
    "        competitor_df,\n",
    "        competitor_col=competitor_col,\n",
    "        our_df=our_df,\n",
    "        our_col=our_col,\n",
    "        top_k=top_k,\n",
    "        threshold=threshold,\n",
    "        encoder_batch=8,\n",
    "        reranker_batch=16,\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    # Analysis\n",
    "    matcher.analyze_results(results)\n",
    "    \n",
    "    # Save\n",
    "    results.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nüíæ Saved: {output_csv}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Check GPU status\n",
    "    check_gpu_status()\n",
    "\n",
    "    # 2. Load files\n",
    "    print(\"\\nüìÇ LOADING DATA:\")\n",
    "    try:\n",
    "        # Note: If using semicolon separator, add argument: sep=';'\n",
    "        df_competitor = pd.read_csv(\"competitor_products.csv\") \n",
    "        df_our = pd.read_csv(\"our_products.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: Files not found.\")\n",
    "        print(\"   Ensure 'competitor_products.csv' and 'our_products.csv' are in the script directory.\")\n",
    "        exit()\n",
    "\n",
    "    # 3. Column Analysis (Helps determine what to configure)\n",
    "    print(\"\\nüîç COLUMN ANALYSIS:\")\n",
    "    print(f\"Competitor file (df_competitor): {df_competitor.columns.tolist()}\")\n",
    "    print(f\"Our file (df_our):               {df_our.columns.tolist()}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # =========================================================================\n",
    "    # ‚öôÔ∏è CONFIGURATION (EDIT THIS BLOCK)\n",
    "    # =========================================================================\n",
    "    \n",
    "    # ‚ùó Enter exact column names for product names here\n",
    "    COMPETITOR_COL_NAME = 'name'   # Column name in competitor_products.csv\n",
    "    OUR_COL_NAME = 'name'          # Column name in our_products.csv\n",
    "\n",
    "    # Performance Settings\n",
    "    USE_FP16 = True                # True for RTX 3050 (saves memory)\n",
    "    BATCH_SIZE_ENCODER = 8         # Use 4 or 8 for 4GB VRAM\n",
    "    BATCH_SIZE_RERANKER = 16       # Use 16 for 4GB VRAM\n",
    "    TOP_K_CANDIDATES = 5           # Number of candidates to rerank\n",
    "\n",
    "    # =========================================================================\n",
    "\n",
    "    # 4. Initialize Class\n",
    "    # cache_dir can be specified to avoid re-encoding our products on next run\n",
    "    matcher = ProductMatcherGPU(\n",
    "        use_fp16=USE_FP16, \n",
    "        cache_dir='./cache_products'\n",
    "    )\n",
    "    \n",
    "    # 5. Start Matching\n",
    "    try:\n",
    "        results = matcher.match_products(\n",
    "            competitor_df=df_competitor,\n",
    "            competitor_col=COMPETITOR_COL_NAME,\n",
    "            our_df=df_our,\n",
    "            our_col=OUR_COL_NAME,\n",
    "            top_k=TOP_K_CANDIDATES,\n",
    "            threshold=0.5,             # Confidence threshold (0.0 - 1.0)\n",
    "            encoder_batch=BATCH_SIZE_ENCODER,\n",
    "            reranker_batch=BATCH_SIZE_RERANKER,\n",
    "            use_cache=True             # Use cache for speedup\n",
    "        )\n",
    "\n",
    "        # 6. Analyze and Save\n",
    "        matcher.analyze_results(results, show_examples=5)\n",
    "        \n",
    "        output_file = 'results_gpu_matched.csv'\n",
    "        results.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nüíæ Results saved to file: {output_file}\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"\\n‚ùå COLUMN ERROR: Column not found {e}\")\n",
    "        print(\"   Check the 'CONFIGURATION' block and compare names with 'COLUMN ANALYSIS' output.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå An error occurred: {e}\")\n",
    "    finally:\n",
    "        # Clear GPU memory at the end\n",
    "        ProductMatcherGPU.clear_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889a26c-fd1d-4cd1-9f5a-ce0df76ed9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
